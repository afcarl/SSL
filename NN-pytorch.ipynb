{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST\n",
    "Here we load the dataset and create data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.MNIST('../data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "test_ds = datasets.MNIST('../data', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "#batch_size = 5 # for testing\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "  File \"/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "train_dl = iter(train_loader)\n",
    "x, y = next(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper method (from fast.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img, title=None):\n",
    "    plt.imshow(img, interpolation='none', cmap=\"gray\")\n",
    "    if title is not None: plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first from torch to numpy\n",
    "X = x.numpy(); Y = y.numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f92df169390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(X[0][0], Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]\n",
      " [-0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296  1.0268056\n",
      "   2.6051068   2.7960303   1.5995764   0.21219873 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296 -0.42421296\n",
      "  -0.42421296 -0.42421296 -0.42421296 -0.42421296]]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][0][:4][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the number of neurons in the hidden unit\n",
    "def get_model(M = 300):\n",
    "    net = nn.Sequential(nn.Linear(28*28, M),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(M, 10))\n",
    "    return net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, test_loader, num_epochs, model, optimizer):\n",
    "    model.train()\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            batch = images.shape[0] # size of the batch\n",
    "            # Convert torch tensor to Variable, change shape of the input\n",
    "            images = Variable(images.view(-1, 28*28)).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "        \n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            total += batch\n",
    "            sum_loss += batch * loss.data[0]\n",
    "\n",
    "                \n",
    "        train_loss = sum_loss/total\n",
    "        val_acc, val_loss = model_accuracy_loss(model, test_loader)\n",
    "    return val_acc, val_loss, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy_loss(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images.view(-1, 28*28)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        outputs = model(images)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        sum_loss += labels.size(0)*loss.data[0]\n",
    "        total += labels.size(0)\n",
    "        correct += pred.eq(labels.data).cpu().sum()\n",
    "    return 100 * correct / total, sum_loss/ total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.75, 2.340800587081909)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = get_model()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "model_accuracy_loss(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Valid Accuracy: 94.1800, Valid Loss: 0.2170\n",
      "Epoch [2/2], Valid Accuracy: 95.7100, Valid Loss: 0.1757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(95.71, 0.1756688919067383, 0.2437614460835854)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_loader, test_loader, num_epochs=2, model=net, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tuning the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "val_acc = []\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "lr_list =  [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "for i in range(len(lr_list)):\n",
    "    print(i)\n",
    "    net = get_model()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr_list[i])\n",
    "    a,b,c = train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)\n",
    "    val_acc.append(a)\n",
    "    val_loss.append(b)\n",
    "    train_loss.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(param, val_acc, val_loss, train_loss):\n",
    "    '''\n",
    "    Print the table containing the list of parameters and corresponding accuracy and losses\n",
    "    '''\n",
    "    M_tuning = pd.concat([pd.Series(param), pd.Series(val_acc), pd.Series(val_loss), pd.Series(train_loss)], axis = 1)\n",
    "    M_tuning.columns = ['value', 'val_acc', 'val_loss', 'train_loss']\n",
    "    print(M_tuning.sort_values(by=['val_acc'], ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     value  val_acc  val_loss  train_loss\n",
      "3  0.00100    98.00  0.090457    0.055628\n",
      "4  0.00010    97.73  0.079588    0.159621\n",
      "2  0.01000    94.57  0.263072    0.184763\n",
      "5  0.00001    92.69  0.259403    0.470338\n",
      "0  1.00000    11.40  2.685377   11.867146\n",
      "1  0.10000    11.13  2.284327    2.184936\n"
     ]
    }
   ],
   "source": [
    "print_table(lr_list, val_acc, val_loss, train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that we get the best validation accuracy when `learning rate = 0.001` and the second best when `learning rate = 0.0001`. For interpolating between the 2 we choose from 0.0001, 0.0003, 0.0005, 0.0007, 0.0009, 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "val_acc = []\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "lr_list =  [0.0001, 0.0003, 0.0005, 0.0007, 0.0009, 0.001]\n",
    "for i in range(len(lr_list)):\n",
    "    print(i)\n",
    "    net = get_model()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr_list[i])\n",
    "    a,b,c = train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)\n",
    "    val_acc.append(a)\n",
    "    val_loss.append(b)\n",
    "    train_loss.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    value  val_acc  val_loss  train_loss\n",
      "1  0.0003    98.08  0.064883    0.083020\n",
      "3  0.0007    98.01  0.080329    0.059207\n",
      "2  0.0005    97.97  0.072722    0.064881\n",
      "5  0.0010    97.79  0.100466    0.056164\n",
      "4  0.0009    97.73  0.086016    0.057586\n",
      "0  0.0001    97.67  0.078883    0.159271\n"
     ]
    }
   ],
   "source": [
    "print_table(lr_list, val_acc, val_loss, train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we see that the best validation accuracy we get is when learning rate is 0.0003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tuning for size of the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = []\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "M = [10, 50, 100, 300, 1000, 2000]\n",
    "for i in range(len(M)):\n",
    "    net = get_model(M = M[i])\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "    a,b,c = train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)\n",
    "    val_acc.append(a)\n",
    "    val_loss.append(b)\n",
    "    train_loss.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value  val_acc  val_loss  train_loss\n",
      "4   1000    95.33  0.245196    0.189891\n",
      "3    300    95.24  0.261072    0.181233\n",
      "2    100    95.21  0.257378    0.183786\n",
      "5   2000    95.13  0.279195    0.197609\n",
      "1     50    94.48  0.284732    0.188099\n",
      "0     10    90.89  0.330724    0.353767\n"
     ]
    }
   ],
   "source": [
    "print_table(M, val_acc, val_loss, train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `M = 1000` gives the best validation accuracy. But if we look at the train loss and validation loss, we can see some extent of overfitting. This is particularly true for M = 100 and 300 where there is a difference of ~0.07 between train and validation loss. In these cases while the train loss is reducing a little, validation loss is increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adding weight decay for regularisation\n",
    "To add L2 regularization use the `weight_decay` argument on the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = []\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "wd = [0, 0.0001, 0.001, 0.01, 0.1, 0.3]\n",
    "for i in range(len(wd)):\n",
    "    net = get_model(M=300)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=wd[i])\n",
    "    a,b,c = train_model(train_loader, test_loader, num_epochs=20, model=net, optimizer=optimizer)\n",
    "    val_acc.append(a)\n",
    "    val_loss.append(b)\n",
    "    train_loss.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    value  val_acc  val_loss  train_loss\n",
      "0  0.0000    98.02  0.122390    0.033368\n",
      "2  0.0010    97.80  0.068106    0.071450\n",
      "1  0.0001    97.74  0.090584    0.040079\n",
      "3  0.0100    96.25  0.142067    0.168552\n",
      "4  0.1000    90.08  0.437822    0.471097\n",
      "5  0.3000    84.72  0.796953    0.817350\n"
     ]
    }
   ],
   "source": [
    "print_table(wd, val_acc, val_loss, train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that weight decay of 0 is giving the best accuracy, but looking at the train and validation loss, I feel that 0.001 may also be a good value for weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adding dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v2(M = 300, p=0):\n",
    "    modules = []\n",
    "    modules.append(nn.Linear(28*28, M))\n",
    "    modules.append(nn.ReLU())\n",
    "    if p > 0:\n",
    "        modules.append(nn.Dropout(p))\n",
    "    modules.append(nn.Linear(M, 10))\n",
    "    return nn.Sequential(*modules).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "val_acc = []\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "dropout = [0.0, 0.2, 0.4, 0.6, 0.8]\n",
    "for i in range(len(dropout)):\n",
    "    print(i)\n",
    "    net2 = get_model_v2(M = 300, p=dropout[i])\n",
    "    optimizer = optim.Adam(net2.parameters(), lr=0.001)\n",
    "    a,b,c = train_model(train_loader, test_loader, num_epochs=20, model=net2, optimizer=optimizer)\n",
    "    val_acc.append(a)\n",
    "    val_loss.append(b)\n",
    "    train_loss.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value  val_acc  val_loss  train_loss\n",
      "2    0.4    98.10  0.107880    0.037922\n",
      "4    0.8    97.99  0.118072    0.054477\n",
      "1    0.2    97.94  0.116377    0.035251\n",
      "3    0.6    97.83  0.130952    0.042333\n",
      "0    0.0    97.81  0.121619    0.033832\n"
     ]
    }
   ],
   "source": [
    "print_table(dropout, val_acc, val_loss, train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we add an optimum dropout rate of 0.4, we get the best accuracy of 98.10%. This implies that in the hidden layer, we drop 40% of the neurons, which gives us this validation accuracy. This seems to be an improvement over the model without any dropout.\n",
    "\n",
    "As compared to the regularised model, dropout has a greater effect on validation accuracy than adding weight decay.\n",
    "\n",
    "Let's see how the model performs when we reduce the learning rate after 10 epochs. For this we make the new learning rate 0.1 * the old learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model1(train_loader, test_loader, num_epochs, model, optimizer):\n",
    "    model.train()\n",
    "    sum_loss = 0.0\n",
    "    total = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch == 10:\n",
    "            optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr']/10\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            batch = images.shape[0] # size of the batch\n",
    "            # Convert torch tensor to Variable, change shape of the input\n",
    "            images = Variable(images.view(-1, 28*28)).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "        \n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            total += batch\n",
    "            sum_loss += batch * loss.data[0]\n",
    "               \n",
    "        train_loss = sum_loss/total\n",
    "        val_acc, val_loss = model_accuracy_loss(model, test_loader)\n",
    "    return val_acc, val_loss, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "val_acc = []\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "dropout = [0.0, 0.2, 0.4, 0.6, 0.8]\n",
    "for i in range(len(dropout)):\n",
    "    print(i)\n",
    "    net2 = get_model_v2(M = 300, p=dropout[i])\n",
    "    optimizer = optim.Adam(net2.parameters(), lr=0.001)\n",
    "    a,b,c = train_model1(train_loader, test_loader, num_epochs=20, model=net2, optimizer=optimizer)\n",
    "    val_acc.append(a)\n",
    "    val_loss.append(b)\n",
    "    train_loss.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value  val_acc  val_loss  train_loss\n",
      "4    0.8    98.53  0.067169    0.049024\n",
      "0    0.0    98.51  0.074585    0.029049\n",
      "3    0.6    98.46  0.068792    0.036483\n",
      "1    0.2    98.44  0.078536    0.029695\n",
      "2    0.4    98.34  0.078277    0.032416\n"
     ]
    }
   ],
   "source": [
    "print_table(dropout, val_acc, val_loss, train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see a significant increase in the validation accuracy when we change the learning rate by a tenth after 10 epochs. In this case, dropout = 0.8 gave the highest validation accuracy of 98.53%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. 3-layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am defining a neural net that by default has a layer size of 300 and then 150 and initial p = 0, i.e. no dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v3(M = 300, p=0):\n",
    "    modules = []\n",
    "    modules.append(nn.Linear(28*28, M))\n",
    "    modules.append(nn.ReLU())\n",
    "    if p > 0:\n",
    "        modules.append(nn.Dropout(p))\n",
    "    modules.append(nn.Linear(M, 150))\n",
    "    modules.append(nn.ReLU())\n",
    "    if p > 0:\n",
    "        modules.append(nn.Dropout(p))    \n",
    "    modules.append(nn.Linear(150, 10))\n",
    "    return nn.Sequential(*modules).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first tune the learning rate for values in 1, 0.1, 0.01, 0.001, 0.0001, 0.00001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "val_acc = []\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "lr_list =  [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "for i in range(len(lr_list)):\n",
    "    print(i)\n",
    "    net = get_model_v3()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr_list[i])\n",
    "    a,b,c = train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)\n",
    "    val_acc.append(a)\n",
    "    val_loss.append(b)\n",
    "    train_loss.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     value  val_acc   val_loss  train_loss\n",
      "3  0.00100    98.17   0.078053    0.059021\n",
      "4  0.00010    97.86   0.069875    0.140632\n",
      "2  0.01000    96.43   0.181646    0.158323\n",
      "5  0.00001    93.10   0.235993    0.457318\n",
      "1  0.10000    11.35   2.307264    2.433067\n",
      "0  1.00000    10.09  29.622449  151.479984\n"
     ]
    }
   ],
   "source": [
    "print_table(lr_list, val_acc, val_loss, train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the best most optimal learning rate is 0.001. Going ahead, we will keep this learning rate fixed.\n",
    "### Tuning the size of the hidden layer\n",
    "Now that we have found the best learning rate, let us fix the number of neurons in the first hidden layer. The size of the second is kept fixed at 150 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "val_acc = []\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "M = [10, 50, 100, 300, 1000, 2000]\n",
    "for i in range(len(M)):\n",
    "    print(i)\n",
    "    net = get_model_v3(M = M[i])\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    a,b,c = train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)\n",
    "    val_acc.append(a)\n",
    "    val_loss.append(b)\n",
    "    train_loss.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value  val_acc  val_loss  train_loss\n",
      "4   1000    98.41  0.072650    0.053336\n",
      "3    300    98.03  0.086128    0.058674\n",
      "2    100    97.60  0.096461    0.072188\n",
      "1     50    97.50  0.091975    0.088726\n",
      "5   2000    97.20  0.156515    0.053430\n",
      "0     10    94.20  0.188927    0.212718\n"
     ]
    }
   ],
   "source": [
    "print_table(M, val_acc, val_loss, train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here M = 1000 seems to give the best validation accuracy. Going forward, we will use this as the number of neurons in the first hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding L2 regularisation \n",
    "After fixing the size of the hidden layer, we try to add regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "val_acc = []\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "wd = [0, 0.0001, 0.001, 0.01, 0.1, 0.3]\n",
    "for i in range(len(wd)):\n",
    "    print(i)\n",
    "    net = get_model_v3(M=1000)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=wd[i])\n",
    "    a,b,c = train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)\n",
    "    val_acc.append(a)\n",
    "    val_loss.append(b)\n",
    "    train_loss.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    value  val_acc  val_loss  train_loss\n",
      "0  0.0000    97.94  0.094614    0.053649\n",
      "1  0.0001    97.93  0.071775    0.060677\n",
      "2  0.0010    97.12  0.089322    0.087501\n",
      "3  0.0100    96.14  0.146321    0.175382\n",
      "4  0.1000    87.92  0.521089    0.564228\n",
      "5  0.3000    65.77  1.223724    1.246326\n"
     ]
    }
   ],
   "source": [
    "print_table(wd, val_acc, val_loss, train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that without weight decay, we get the best validation accuracy. But I feel that the validation loss being much lower at weight decay = 0.0001 also implies that this could be the optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dropout without regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "val_acc = []\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "dropout = [0.0, 0.2, 0.4, 0.6, 0.8]\n",
    "for i in range(len(dropout)):\n",
    "    print(i)\n",
    "    net = get_model_v3(M = 1000, p=dropout[i])\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    a,b,c = train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)\n",
    "    val_acc.append(a)\n",
    "    val_loss.append(b)\n",
    "    train_loss.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value  val_acc  val_loss  train_loss\n",
      "2    0.4    97.97  0.093606    0.066110\n",
      "3    0.6    97.93  0.085355    0.080290\n",
      "0    0.0    97.81  0.100169    0.054286\n",
      "4    0.8    97.80  0.091906    0.121479\n",
      "1    0.2    97.62  0.128524    0.058547\n"
     ]
    }
   ],
   "source": [
    "print_table(dropout, val_acc, val_loss, train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the highest validation accuracy is when dropout rate = 0.4. \n",
    "Let's now combine dropout = 0.4 and weight decay = 0.0001 and see what we get.\n",
    "\n",
    "## Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = get_model_v3(M = 1000, p=0.4)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "val_acc,val_loss,train_loss = train_model(train_loader, test_loader, num_epochs=10, model=net, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy = 98.03, validation loss = 0.06831177916526794, train loss = 0.07197758896847566\n"
     ]
    }
   ],
   "source": [
    "print('Validation accuracy = ' + str(val_acc) + ', validation loss = ' + str(val_loss) + ', train loss = ' + str(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
